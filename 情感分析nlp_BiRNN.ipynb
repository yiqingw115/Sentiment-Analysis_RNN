{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGHoy6KpQDfZ"
   },
   "source": [
    "# COMP5046 Assignment 1\n",
    "*Make sure you change the file name with your unikey.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTf21j_oQIiD"
   },
   "source": [
    "# Readme\n",
    "*If there is something to be noted for the user, please mention here.* \n",
    "\n",
    "*If you are planning to implement a program with Object Oriented Programming style, please check the bottom of the this ipynb file*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXbQohXLKSgO"
   },
   "source": [
    "***Visualising the comparison of different results is a good way to justify your decision.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34DVNKgqQY21"
   },
   "source": [
    "# 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cWUxAQrGlq6"
   },
   "source": [
    "## 1.1. Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr8o7UZxWf3Y"
   },
   "source": [
    "If you want to know how data has been saved in pickle file: see this [ipynb file](https://drive.google.com/file/d/1ZQUVBzgH7N2EbiyE3WTPx7JNe2eRTs36/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7C4snIcNl22",
    "outputId": "6e4fd33e-2b65-4055-d870-7299e2248df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Size of training dataset: 8000\n",
      "Size of testing dataset: 2000\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Sample Data\n",
      "LABEL: neg / SENTENCE: hopeless for tmr :(\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Code to download file into Colaboratory:\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "id = '1lTD6bgRkmwguGAr30v-r0KBPdtnVneLb'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('testing_data.pkl')  \n",
    "\n",
    "id = '1pCUdlZMoj99UZHtqFeza86fvVQfFmDFX'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "downloaded.GetContentFile('training_data.pkl')  \n",
    "\n",
    "import pickle\n",
    "training_data = pickle.load(open(\"training_data.pkl\",\"rb\"))\n",
    "testing_data = pickle.load(open(\"testing_data.pkl\",\"rb\"))\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Size of training dataset: {0}\".format(len(training_data)))\n",
    "print(\"Size of testing dataset: {0}\".format(len(testing_data)))\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(\"Sample Data\")\n",
    "print(\"LABEL: {0} / SENTENCE: {1}\".format(training_data[0][0], training_data[0][1]))\n",
    "print(\"------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9gBSgBCQh24"
   },
   "source": [
    "## 1.2. Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RdKI8E2KRwe"
   },
   "source": [
    "\n",
    "\n",
    "*You are required to describe which data preprocessing techniques were conducted with justification of your decision. *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf7jyJ2JewHl"
   },
   "source": [
    "The major techniques in the following part for preprocessing are the regular expression method for check and filter strings from documents, and tokenize methods for spliting each word as tokens, and use lemmatization method for get the base form of words, then use stemming method reduce tokens to their stems in information retrieval.\n",
    "\n",
    "\n",
    "---\n",
    "This is a text dataset comes from Twitter composed by text included different punctuation, different language, and soome unimformative words, like the web links, emails links, mentions(@) punctuations and numbers. I would remove these information for better training samples. First, lowercase all the tokens, remove emails, webs and mentions links. I would retain hasgtags because sometimes it might also contain the sentitive information. Besides, although not always, but most of time some emoji might contain information as well, like 'heart', ':)' simle might stand for a positive emotion, so I would use [emoji] package to transfer image emoji to words forms, replace word emoji like ':)' as word 'simleface'. [TweetTokenizer] can helps to split these word emoji. Third, I would try to only retain Enlish lowercase words for training sample, so I use language code on regular expression method to remove other languages. Finally, I would remove stop words which also not contain much information. Stemming is for normalize spelling words, because the text on the Internet might not much well-spelling and reduce the number of different words. Lemmatization ir for reduce the number of dimension for belowing word embedding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emyl1lWxGr12",
    "outputId": "f86458cd-cc54-4fab-d9c8-3b200f0b5a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Please comment your code\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXdwRfSFDbhZ",
    "outputId": "b5123a65-4d1e-4984-fabf-8b7091d8f63a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train duplicated: 0 test duplicated: 0\n",
      "0    0\n",
      "1    0\n",
      "dtype: int64 0    0\n",
      "1    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check duplicated print(\"train duplicated:\", np.sum(pd.DataFrame(training_data).duplicated()), \"test duplicated:\", np.sum(pd.DataFrame(testing_data).duplicated()))\n",
    "\n",
    "# check/remove na print(np.sum(pd.DataFrame(training_data).isna()), np.sum(pd.DataFrame(testing_data).isna()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AIcaryYoPYtj"
   },
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√',  '╲', '╱', ]\n",
    "\n",
    "def remove_punctuation(x):\n",
    "  \"\"\"\n",
    "  Input: single word\n",
    "  Output: justified word\n",
    "  \"\"\"\n",
    "  x = str(x)\n",
    "  for punct in puncts:\n",
    "      if punct == x:\n",
    "          x = x.replace(punct, '')\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s38l9twkWCSh",
    "outputId": "24bbd039-00b8-4180-a6a1-f434e5d4b695"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "# Prepare Tokenisation\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# Remove username handles and Normalize word lengthening\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len = True)\n",
    "# Prepare removing stop words\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = sw.words()\n",
    "# Prepare NLTK Wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "def Clean(sentence):\n",
    "  \"\"\"\n",
    "  Input sentence: 'hopeless for tmr :(' (e.g.)\n",
    "  Output tokens: ['hopeless', 'tmr', 'cryface'] (e.g.)\n",
    "\n",
    "  \"\"\"\n",
    "  # Converting to lowercase\n",
    "  sentence = str(sentence[1]).lower()\n",
    "  # Remove links\n",
    "  remove_link = re.sub(r'http\\S+', '', sentence)\n",
    "  # Remove email\n",
    "  remove_email = re.sub(r'@', '', remove_link)\n",
    "  # Remove other language instead of English\n",
    "  remove_lang = re.sub(u'\\u0061-\\u007a', '', remove_email)\n",
    "  # Remove image emoji and change into words\n",
    "  remove_emoji = emoji.demojize(remove_lang)\n",
    "  # Remove numbers\n",
    "  remove_num = re.sub('[0-9]+', \"\", remove_emoji)\n",
    "\n",
    "  # Tokenisation\n",
    "  tokens = tknzr.tokenize(remove_num)\n",
    "  # Remove stop words\n",
    "  filtered_sentence = [w for w in tokens if not w in stop_words]\n",
    "  # lemmatizer\n",
    "  lemm = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "  # Remove single punctuation\n",
    "  repl_punc = [remove_punctuation(x) for x in lemm]\n",
    "  remove_punc = [x for x in repl_punc if x != '']\n",
    "  # stemmer\n",
    "  stemm = [stemmer.stem(w) for w in remove_punc]\n",
    "  # change word emoji into word describtion\n",
    "  sent = [w.replace(r':(',\"cryface\").replace(r':-(',\"cryface\").replace(r':)',\"simleface\").replace(r':-)',\"simleface\") for w in stemm]\n",
    "  \n",
    "    \n",
    "  return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qOoCMVXMT8uT"
   },
   "outputs": [],
   "source": [
    "def Encode_y(dataset):\n",
    "  \"\"\"\n",
    "  Input the original label: \"pos\",\"neg\"\n",
    "  Output: \"pos\" <- 2, \"neg\" <- 1.\n",
    "  \"\"\"\n",
    "  labels = []\n",
    "  for i in range(len(dataset)):\n",
    "    label = dataset[i][0]\n",
    "    label = label.replace(r'neg','1').replace(r'pos','2')\n",
    "    \n",
    "    labels.append(label)\n",
    "\n",
    "  for x in labels:\n",
    "    if type(x) == 'str':\n",
    "      ptint(x)\n",
    "    else:\n",
    "      pass\n",
    "  encoded = [int(x) for x in labels]\n",
    "\n",
    "  return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n35vxU4yfV9Y"
   },
   "outputs": [],
   "source": [
    "train_x = [Clean(sentences) for sentences in training_data]\n",
    "train_y = Encode_y(training_data)\n",
    "\n",
    "test_x = [Clean(sentences) for sentences in testing_data]\n",
    "test_y = Encode_y(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIu_lkJwQ55g"
   },
   "source": [
    "# 2 - Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daDvAftceIvr"
   },
   "source": [
    "## 2.1. Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbzm-NWBTmM-"
   },
   "source": [
    "*You are required to describe which model was implemented (i.e. Word2Vec with CBOW, FastText with SkipGram, etc.) with justification of your decision *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06EqnRegnC7y"
   },
   "source": [
    "  First at all, I choose FastText to do word embedding in this part. This is \n",
    "becasue compared with Word2Vec and CBOW are predicting context, FastText is \n",
    "predicting the labels. In this case, out goal is to get the classification of sensitive label, so I choose FastText as the word embedding technique. In addition, word2vec has a huge limitation，it cannot infer the unfamiliar word vector if using the other corpus. This is OOV problem.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "  However, FastText trained a large corpus would take a long time. In a ddition, if we train Word2Vec by corpus with the raw training data and testing data, the OOV problem can be avoid.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "  Therefore, I will train Word2Vec with TED talk data (processed from Lab02) and Twitter dataset as the word embedding method at the following part.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3cM4rlYkHefJ"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "\n",
    "import pprint\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import FastText\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# test\n",
    "# ft_sg_model = FastText(train_x, size=100, window=5, min_count=5, workers=2, sg=1)\n",
    "# just for test: result=ft_sg_model.wv.most_similar(\"hopeless\") pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXgFpxIgl-_G"
   },
   "source": [
    "### 2.1.1. Data Preprocessing for Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJrVHGYSmYMg"
   },
   "source": [
    "*You are required to describe which preprocessing techniques were used with justification of your decision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nweSqnt4qN2F"
   },
   "source": [
    "The only preprocessing in this part is for TED talk dataset. First, download it which si a xml file. Then, remove the unimformational words. Finally, get a token form list by each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "VLQL7o88pcXe"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# ref: Lab 02 \n",
    "# Download TED talk: about 55s nedded\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# For parsing our XML data\n",
    "from lxml import etree \n",
    "# Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
    "downloaded = drive.CreateFile({'id':id}) \n",
    "\n",
    "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
    "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "# Getting contents of <content> tag from the xml file\n",
    "target_text = etree.parse(targetXML)\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# Tokenising the sentence to process it by using NLTK library\n",
    "sent_text=sent_tokenize(content_text)\n",
    "\n",
    "# Removing punctuation and changing all characters to lower case\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "# Tokenising each sentence to process individual word\n",
    "ted_sentences=[]\n",
    "ted_sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
    "\n",
    "#test:    print(ted_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9nG0iSZcM0Eo",
    "outputId": "4f2fd883-e0eb-4021-93fc-02970c0cbeea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "nltk.download('twitter_samples')\n",
    "twitter_samples.fileids()\n",
    "tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "tweets_tokens = twitter_samples.tokenized('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "y1TmJ1tyN-ee"
   },
   "outputs": [],
   "source": [
    "def Cleantw(sent):\n",
    "  clean_tweets = []\n",
    "  for i in sent:\n",
    "    #print(i.lower())\n",
    "    x = re.sub(r'http\\S+', '', i.lower())\n",
    "    x = re.sub(r'@', '', x)\n",
    "    x = re.sub(u'\\u0061-\\u007a', '', x)\n",
    "    # Remove image emoji and change into words\n",
    "    x = emoji.demojize(x)\n",
    "    # Remove numbers\n",
    "    x = re.sub('[0-9]+', '', x)\n",
    "    x = remove_punctuation(x)\n",
    "    if x not in stop_words:\n",
    "      \n",
    "      x = lemmatizer.lemmatize(x)\n",
    "      x = stemmer.stem(x)\n",
    "      clean_tweets.append(x)\n",
    "      \n",
    "  cleaned_tweets = []\n",
    "  for i in clean_tweets:\n",
    "    if i != '':\n",
    "      cleaned_tweets.append(i)\n",
    "  return cleaned_tweets    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "sbZj7bNORbF0"
   },
   "outputs": [],
   "source": [
    "cleantw = [Cleantw(sent) for sent in tweets_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkjSpODrRz-F",
    "outputId": "aaf33b91-7267-4c59-c49d-f9f5bd80919e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 0,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleantw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhAgWf_AmbZ8"
   },
   "source": [
    "### 2.1.2. Build Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ8rU7JbiBVS"
   },
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0Eo3hpDrwSM"
   },
   "source": [
    "There are many parameters need to set up in [gensim.models.word2vec.Word2Vec]. \n",
    "\n",
    "{\n",
    "  \n",
    "  sg: for set up algorithm, 0 is cbow, 1 is skip-gram \n",
    "\n",
    "  size: the dimension of vetor, 100 is default.\n",
    "\n",
    "  alpha: learning rate, 0.025 is default, set double 0.05 for shorter running time\n",
    "\n",
    "  min_alpha: minimum learning rate, 0.0001 is defualt\n",
    "\n",
    "  window: the distance between current word and predicted word in a stence, 5 is default.\n",
    "\n",
    "  min_count: can cut off the dictionary, if the frequence of words less then min_count, it would be drop off, 5 is default, however, the dataset we used is not very large, I want more accurate so I set 1 here. The most important point is that 1 can help avoid OOV problem.\n",
    "\n",
    "  seed: randomly begin. set any number.\n",
    "\n",
    "  workers: control the number of parallel, 3 as default.\n",
    "\n",
    "  negative: if larger than 0, negative sampling would be took for noise words. 5 is default. I would use hierarchical softmax method instead of negative sampling, so set 0 here.\n",
    "\n",
    "  hs: 0: negative sampling, 1:hierarchical softmax. Hierarchical is based on the binary tree, so high-frequency words need less time can be found according to greedy optimization. Therefore, set 1.\n",
    "\n",
    "  compute_loss: TRUE, because I want to use [get_latest_training_loss()] below to check loss.\n",
    "\n",
    "  iter: 5 is default. I want more train times, so set 10.\n",
    "\n",
    "  cbow_mean: 0 is the sum of context, 1 is default that use mean value, becasue I use cbow algorithm here, so need to set this parameter.\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "ref: https://radimrehurek.com/gensim/models/word2vec.html\n",
    "(code) https://github.com/RaRe-Technologies/gensim/issues/2039"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "Jftz99LSvd8v"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# hyperparameters prepare\n",
    "Dimension_embeddings = 100\n",
    "word2vec_params = {\n",
    "   'sg': 0,       # CBOW\n",
    "   \"size\": Dimension_embeddings,\n",
    "   \"alpha\": 0.5,\n",
    "   \"min_alpha\": 0.001,\n",
    "   'window': 10,\n",
    "   'min_count': 1,\n",
    "   'seed': 1,\n",
    "   \"workers\": 4,\n",
    "   \"negative\": 0,\n",
    "   \"hs\": 1,       # hierarchical softmax\n",
    "   'compute_loss': True,\n",
    "   'iter': 10,\n",
    "   'cbow_mean':1,\n",
    "}\n",
    "model = Word2Vec(**word2vec_params)\n",
    "model.build_vocab(cleantw+ted_sentences+train_x+test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNys5HOdISK-"
   },
   "source": [
    "### 2.1.3. Train Word Embeddings Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "Ae8i7Z2kIef-",
    "outputId": "53aa26d8-f791-4114-d4e2-1c4d50f3dbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 69473.6875 0.5\n",
      "1 71894.0625 0.4501\n",
      "2 72277.625 0.4002\n",
      "3 71113.625 0.3503\n",
      "4 69156.6953125 0.3004\n",
      "5 68274.3359375 0.2505\n",
      "6 67462.640625 0.2006\n",
      "7 70214.65625 0.1507\n",
      "8 71730.6328125 0.1008\n",
      "9 66688.1953125 0.0509\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Please comment your code\n",
    "# about 53s needed\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "learning_rate = 0.5\n",
    "step_size = (0.5 - 0.001) / 10\n",
    "\n",
    "W2V_x = []\n",
    "W2V_y = []\n",
    "\n",
    "for i in range(10):\n",
    "    trained_word_count, raw_word_count = model.train(cleantw+ted_sentences+train_x+test_x, compute_loss=True,\n",
    "                                                     start_alpha=learning_rate,\n",
    "                                                     end_alpha=learning_rate,\n",
    "                                                     total_examples=model.corpus_count,\n",
    "                                                     epochs=5)\n",
    "    loss = model.get_latest_training_loss()\n",
    "    \n",
    "    print(i, loss, learning_rate)\n",
    "    W2V_x.append(i) # for plot\n",
    "    W2V_y.append(loss) # for plot\n",
    "    \n",
    "    learning_rate -= step_size\n",
    "\n",
    "plt.title(\"Training performance\")\n",
    "plt.plot(W2V_x, W2V_y, label='Training loss')\n",
    "plt.legend()\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "ji4cTg-QW1Qv"
   },
   "outputs": [],
   "source": [
    "#save the model\n",
    "model.save(\"word2vec.model\")\n",
    "\n",
    "#load the model\n",
    "loaded_w2vmodel = Word2Vec.load(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "sb125dDru373"
   },
   "outputs": [],
   "source": [
    "def get_embedding(sent):\n",
    "  Word2Vec_embedding = []\n",
    "  for token in sent:\n",
    "    Word2Vec_embedding.append(loaded_w2vmodel.wv[token])\n",
    "  return np.array(Word2Vec_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "jYo5c_MEoafZ"
   },
   "outputs": [],
   "source": [
    "train_W2V_embedding = [get_embedding(train_x[token]) for token in range(len(train_x))]\n",
    "test_W2V_embedding = [get_embedding(test_x[token]) for token in range(len(test_x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0ap96aeGlIk"
   },
   "source": [
    "## 2.2. Lexicon Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d16v3oKaGlI0"
   },
   "source": [
    "### 2.2.1. Lexicon-based Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AKbLnN-3GlI1"
   },
   "source": [
    "*[Optional] You are required to describe why you would like to use more than one-dimensional embedding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2CUCL1cGlI2"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "###\n",
    "# if you want to use uploaded file, run this chunk. I am using the corpus from nltk, so run and import in the next chunk.\n",
    "###\n",
    "\n",
    "#lexi_negative = open('/content/opinion-lexicon-English/negative-words.txt', 'r', encoding = \"ISO-8859-1\") \n",
    "#lexi_positive = open('/content/opinion-lexicon-English/positive-words.txt', 'r', encoding = \"ISO-8859-1\") \n",
    "#negative = lexi_negative.read()\n",
    "#positive = lexi_positive.read()\n",
    "#negative_list = [negative.split( )][0]\n",
    "#positive_list = [positive.split( )][0]\n",
    "\n",
    "### this step is to find the introduction words \n",
    "### and get the begin index of usable words\n",
    "\n",
    "#tmp_nidx = []\n",
    "#for i in range(len(negative_list)):\n",
    "#  if \";\" in negative_list[i]:\n",
    "    #print(i)\n",
    "#    tmp_nidx.append(i)\n",
    "#max_n = np.max(tmp_nidx)\n",
    "\n",
    "#tmp_pidx = []\n",
    "#for i in range(len(positive_list)):\n",
    "#  if \";\" in positive_list[i]:\n",
    "    #print(i)\n",
    "#    tmp_pidx.append(i)\n",
    "#max_p = np.max(tmp_pidx)\n",
    "\n",
    "#negative_lexi = negative_list[int(max_n + 1):]\n",
    "\n",
    "#positive_lexi = positive_list[int(max_p + 1):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VA8JW6wzoScC",
    "outputId": "e981ea54-e0ff-49b4-8482-ef0ee534cd57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "lexi_negative: 4783 lexi_positive: 2006\n"
     ]
    }
   ],
   "source": [
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "lexi_negative = opinion_lexicon.negative()\n",
    "lexi_positive = opinion_lexicon.positive()\n",
    "print(\"lexi_negative:\",len(lexi_negative), \"lexi_positive:\",len(lexi_positive)) # lexi_negative: 4783 lexi_positive: 2006 is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "QnQOiMKBdpL9"
   },
   "outputs": [],
   "source": [
    "stemm_negative_lexi = [stemmer.stem(w) for w in lexi_negative]\n",
    "stemm_positive_lexi = [stemmer.stem(w) for w in lexi_positive]\n",
    "# This is because the lexi corpus words are official version, \n",
    "# so in order to match the stemmized processed tokens, it should be stemmized as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "d_NP_WM2eh0j"
   },
   "outputs": [],
   "source": [
    "# test: train_x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "ZN7gsDMbPqUU"
   },
   "outputs": [],
   "source": [
    "def Encode_lexi(token_list):\n",
    "  lexi_token = []\n",
    "  for token in token_list:\n",
    "    if token in stemm_negative_lexi:\n",
    "      lexi_token.append(1) # negative(1)\n",
    "    elif token in stemm_positive_lexi:\n",
    "      lexi_token.append(2) # positive(2)\n",
    "    else:\n",
    "      lexi_token.append(0) # not_exist(0)\n",
    "\n",
    "  return lexi_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A5y0N85rSyEa",
    "outputId": "b662cfc3-a0b7-411b-bf93-ffd2352d1f3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000,)"
      ]
     },
     "execution_count": 157,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lexi_code = [Encode_lexi(train_x[document]) for document in range(len(train_x))]\n",
    "test_lexi_code = [Encode_lexi(test_x[document]) for document in range(len(test_x))]\n",
    "np.array(train_lexi_code).shape # because now each document is different length, so the dimension is not shown here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMrVwFE4Wzks"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlCeWT8eeLnd"
   },
   "source": [
    "## 2.3. Bi-directional RNN Sequence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwA-NN3EJ4Ig"
   },
   "source": [
    "### 2.3.1. Apply/Import Word Embedding and Lexicon Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "g7PKX1gIePA2"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "def Padding(embeddinged):\n",
    "  length = int(np.mean([len(i) for i in embeddinged]))\n",
    "  max_length = int(np.max([len(i) for i in embeddinged]))\n",
    "  zeros = np.zeros((max_length, Dimension_embeddings))\n",
    "\n",
    "  padding = []\n",
    "  for sent in embeddinged:\n",
    "    if len(sent) >= length:\n",
    "      padding.append(list(np.array(sent)[:length]))\n",
    "\n",
    "    else:\n",
    "      padding.append(list(np.vstack((zeros[:int(length - len(sent))],sent))))\n",
    "\n",
    "    np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
    "  padding = np.array(padding)\n",
    "\n",
    "  return padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "yoitUWYEr7_8"
   },
   "outputs": [],
   "source": [
    "train_W2V = Padding(train_W2V_embedding)\n",
    "test_W2V = Padding(test_W2V_embedding)  # test: print(train_W2V.shape, test_W2V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "wzr5s8T5sv5c"
   },
   "outputs": [],
   "source": [
    "def Paddinglexi(embedding):\n",
    "  length = int(np.mean([len(i) for i in embedding]))\n",
    "  max_length = int(np.max([len(i) for i in embedding]))\n",
    "  \n",
    "  zeros = [0]*max_length\n",
    "\n",
    "  lexi_padding = []\n",
    "  for sent in embedding:\n",
    "    if len(sent) >= length:\n",
    "      lexi_padding.append(sent[:length])\n",
    "    else:\n",
    "      lexi_padding.append(sent+zeros[:int(length-len(sent))])\n",
    "\n",
    "  lexi_padding = np.array(lexi_padding)\n",
    "\n",
    "  return lexi_padding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "5bBH0MfzuCSU"
   },
   "outputs": [],
   "source": [
    "train_lexi_padding = Paddinglexi(train_lexi_code)\n",
    "test_lexi_padding = Paddinglexi(test_lexi_code)  # test: print(train_lexi_padding.shape, test_lexi_padding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "-s_r26KguVlx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "def Concatenate(padding, padding_lexi, length, num_vector):\n",
    "  \n",
    "  concatenate = []\n",
    "  for lexi in range(len(padding_lexi)):\n",
    "    embedding = nn.Embedding(length,num_vector)\n",
    "    input = torch.LongTensor(padding_lexi[lexi])\n",
    "    concat = torch.cat((torch.from_numpy(padding[lexi]),embedding(input)), 0)\n",
    "    #print(concat.shape)\n",
    "    #print(concat.detach().numpy().shape)\n",
    "    concatenate.append(concat.detach().numpy())\n",
    "  concatenate = np.array(concatenate)\n",
    "  return concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "2xSSB-g4vlIq"
   },
   "outputs": [],
   "source": [
    "train_input = Concatenate(train_W2V, train_lexi_padding, 7, Dimension_embeddings)\n",
    "test_input = Concatenate(test_W2V, test_lexi_padding, 7, Dimension_embeddings)  # test: print(train_input.shape, test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "4o_lAhl8er1X"
   },
   "outputs": [],
   "source": [
    "train_y = np.array(train_y)\n",
    "test_y = np.array(test_y)  # test: print(train_y.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpYCL17JKZxl"
   },
   "source": [
    "### 2.3.2. Build Sequence Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R204UIyDKhZ4"
   },
   "source": [
    "*You are required to describe how hyperparameters were decided with justification of your decision.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "13eCtR_SLUG6"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "ixYfGTLOa9w6"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_directions = 2\n",
    "num_layers = 2  # 2 Bi-RNN here \n",
    "\n",
    "class Bi_RNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Bi_RNN_Model, self).__init__()\n",
    "\n",
    "        # input of shape (seq_len, batch, input_size)\n",
    "        # h_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
    "        # bidirectional – If True, becomes a bidirectional RNN. Default: False\n",
    "        # If the RNN is bidirectional, num_directions should be 2, else it should be 1.\n",
    "\n",
    "        self.rnn = nn.RNN(n_input, n_hidden, num_layers = num_layers, batch_first=True, dropout=0.2, bidirectional = True)     \n",
    "        self.linear = nn.Linear(n_hidden*num_directions, n_class+1)\n",
    "\n",
    "    def forward(self, x):        \n",
    "\n",
    "        # output of shape (seq_len, batch, num_directions * hidden_size)\n",
    "        # h_n of shape (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        rnn_output, h_n = self.rnn(x)\n",
    "\n",
    "        # got (batch, hidden_size) and pop the last hidden states\n",
    "        # rnn_output[:,-1,:128] <-- the last output of forward\n",
    "        # rnn_output[:,0,-128:] <-- the last output of backward\n",
    "\n",
    "        x_last = torch.cat((rnn_output[:,-1,:n_hidden],rnn_output[:,0,-n_hidden:]),dim = 1)\n",
    "        \n",
    "        output = self.linear(x_last)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "F1F8Ej4ybIpZ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# the length of each sequence = \n",
    "seq_length = int(train_input.shape[1])\n",
    "# the input feature dimension = \n",
    "n_input = int(train_input.shape[2])\n",
    "# the number of class = \n",
    "n_class = 2\n",
    "\n",
    "# the hyperparameters\n",
    "n_hidden = 128\n",
    "batch_size =  50\n",
    "total_epoch = 50\n",
    "learning_rate = 0.003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6BaOiaGRLW7R"
   },
   "source": [
    "### 2.3.3. Train Sequence Model\n",
    "\n",
    "Note that it will not be marked if you do not display the Training Loss and the Number of Epochs in the Assignment 1 ipynb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVQnUSX1LZ6C"
   },
   "outputs": [],
   "source": [
    "# Please comment your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "id": "9-gP1FeziuOB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                    train_input, train_y, test_size=0.33, random_state=1442)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RpV2sE6ib3mD",
    "outputId": "fb150b2f-37e0-41a9-d9e8-adced6ada8b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 63.69012, train_acc: 0.90\n",
      "Epoch: 2, train loss: 36.55801, train_acc: 0.80\n",
      "Epoch: 3, train loss: 34.26443, train_acc: 0.60\n",
      "Epoch: 4, train loss: 36.21677, train_acc: 0.90\n",
      "Epoch: 5, train loss: 31.35741, train_acc: 0.90\n",
      "Epoch: 6, train loss: 32.10292, train_acc: 0.90\n",
      "Epoch: 7, train loss: 31.33810, train_acc: 0.90\n",
      "Epoch: 8, train loss: 26.84082, train_acc: 0.90\n",
      "Epoch: 9, train loss: 27.82705, train_acc: 1.00\n",
      "Epoch: 10, train loss: 28.36945, train_acc: 1.00\n",
      "Epoch: 11, train loss: 27.48341, train_acc: 1.00\n",
      "Epoch: 12, train loss: 26.72350, train_acc: 1.00\n",
      "Epoch: 13, train loss: 23.10744, train_acc: 1.00\n",
      "Epoch: 14, train loss: 20.79539, train_acc: 1.00\n",
      "Epoch: 15, train loss: 17.99434, train_acc: 1.00\n",
      "Epoch: 16, train loss: 16.60524, train_acc: 1.00\n",
      "Epoch: 17, train loss: 15.59289, train_acc: 0.90\n",
      "Epoch: 18, train loss: 18.00295, train_acc: 1.00\n",
      "Epoch: 19, train loss: 15.44090, train_acc: 1.00\n",
      "Epoch: 20, train loss: 14.85461, train_acc: 1.00\n",
      "Epoch: 21, train loss: 14.74645, train_acc: 1.00\n",
      "Epoch: 22, train loss: 11.78453, train_acc: 1.00\n",
      "Epoch: 23, train loss: 8.87320, train_acc: 1.00\n",
      "Epoch: 24, train loss: 9.83491, train_acc: 0.90\n",
      "Epoch: 25, train loss: 8.36253, train_acc: 1.00\n",
      "Epoch: 26, train loss: 8.82375, train_acc: 0.90\n",
      "Epoch: 27, train loss: 15.63670, train_acc: 1.00\n",
      "Epoch: 28, train loss: 9.81500, train_acc: 1.00\n",
      "Epoch: 29, train loss: 8.59180, train_acc: 1.00\n",
      "Epoch: 30, train loss: 5.88193, train_acc: 1.00\n",
      "Epoch: 31, train loss: 3.66744, train_acc: 1.00\n",
      "Epoch: 32, train loss: 2.47294, train_acc: 1.00\n",
      "Epoch: 33, train loss: 3.11172, train_acc: 1.00\n",
      "Epoch: 34, train loss: 2.64584, train_acc: 1.00\n",
      "Epoch: 35, train loss: 2.88454, train_acc: 1.00\n",
      "Epoch: 36, train loss: 2.50909, train_acc: 1.00\n",
      "Epoch: 37, train loss: 3.10722, train_acc: 1.00\n",
      "Epoch: 38, train loss: 3.34481, train_acc: 1.00\n",
      "Epoch: 39, train loss: 4.43514, train_acc: 1.00\n",
      "Epoch: 40, train loss: 2.88573, train_acc: 1.00\n",
      "Epoch: 41, train loss: 2.51225, train_acc: 1.00\n",
      "Epoch: 42, train loss: 1.85254, train_acc: 1.00\n",
      "Epoch: 43, train loss: 1.96756, train_acc: 1.00\n",
      "Epoch: 44, train loss: 2.99146, train_acc: 1.00\n",
      "Epoch: 45, train loss: 2.43019, train_acc: 1.00\n",
      "Epoch: 46, train loss: 4.21630, train_acc: 1.00\n",
      "Epoch: 47, train loss: 4.83012, train_acc: 1.00\n",
      "Epoch: 48, train loss: 6.33849, train_acc: 1.00\n",
      "Epoch: 49, train loss: 3.92741, train_acc: 1.00\n",
      "Epoch: 50, train loss: 2.95694, train_acc: 1.00\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = Bi_RNN_Model().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Please find which optimizer provides a higher f1\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    train_loss = 0\n",
    "    for ind in range(0,X_train.shape[0],batch_size):\n",
    "        input_batch = X_train[ind:min(ind+batch_size, X_train.shape[0])]\n",
    "        target_batch = y_train[ind:min(ind+batch_size, X_train.shape[0])]\n",
    "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
    "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
    "\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_batch_torch) \n",
    "        loss = criterion(outputs, target_batch_torch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        acc= accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy())\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print('Epoch: %d, train loss: %.5f, train_acc: %.2f'%(epoch + 1, train_loss, acc))\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0G_IHOjt4Hb5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjG1RzDJf-vE",
    "outputId": "6d36470d-d7eb-477c-900e-04853b8e8cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.8164    0.8046    0.8104      1315\n",
      "           2     0.8088    0.8204    0.8145      1325\n",
      "\n",
      "    accuracy                         0.8125      2640\n",
      "   macro avg     0.8126    0.8125    0.8125      2640\n",
      "weighted avg     0.8126    0.8125    0.8125      2640\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Prediction\n",
    "model.eval()\n",
    "outputs = model(torch.from_numpy(X_val).float().to(device)) \n",
    "predicted = torch.argmax(outputs, 1)\n",
    "\n",
    "# classification_report builds a text report showing the main classification metrics\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, predicted.cpu().numpy(),digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4mpRpocePLN"
   },
   "source": [
    "# 3 - Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lbLBzHObsvvM"
   },
   "source": [
    "## 3.1. Word Embedding Evaluation\n",
    "You are to apply Semantic-Syntactic word relationship tests for the trained word embeddings and visualise the result of Semantic-Syntactic word relationship tests.\n",
    "Note that it will not be marked if you do not display it in the ipynb file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSIUsb7qtQEf"
   },
   "source": [
    "(*Please show your empirical evidence*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gPYUJaHMUwl_"
   },
   "outputs": [],
   "source": [
    "# Please comment your code\n",
    "# hyperparameters prepare\n",
    "def getfile(dim, window):\n",
    "  Dimension_embeddings = dim\n",
    "  word2vec_params = {\n",
    "    'sg': 0,       # CBOW\n",
    "    \"size\": Dimension_embeddings,\n",
    "    \"alpha\": 0.05,\n",
    "    \"min_alpha\": 0.0001,\n",
    "    'window': window,\n",
    "    'min_count': 2,\n",
    "    'seed': 1,\n",
    "    \"workers\": 4,\n",
    "    \"negative\": 0,\n",
    "    \"hs\": 1,       # hierarchical softmax\n",
    "    'compute_loss': True,\n",
    "    'iter': 10,\n",
    "    'cbow_mean':1,\n",
    "  }\n",
    "  model = Word2Vec(**word2vec_params)\n",
    "  model.build_vocab(ted_sentences+train_x+test_x)\n",
    "\n",
    "  losses = []\n",
    "  learning_rate = 0.5\n",
    "  step_size = (0.5 - 0.001) / 10\n",
    "\n",
    "  for i in range(10):\n",
    "      trained_word_count, raw_word_count = model.train(ted_sentences+train_x+test_x, compute_loss=True,\n",
    "                                                      start_alpha=learning_rate,\n",
    "                                                      end_alpha=learning_rate,\n",
    "                                                      total_examples=model.corpus_count,\n",
    "                                                      epochs=5)\n",
    "      loss = model.get_latest_training_loss()\n",
    "      losses.append(loss)\n",
    "      print(i, loss, learning_rate)\n",
    "      \n",
    "      learning_rate -= step_size\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "l1Ncn_0pEshw"
   },
   "outputs": [],
   "source": [
    "loaded_w2vmodel.wv.save_word2vec_format('cbow_w2v.txt', binary=False)\n",
    "ofile = \"/content/cbow_w2v.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kp0Or3uhUwfV",
    "outputId": "53ddec13-db17-44b3-dfdf-89d03f5794e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 139253.875 0.5\n",
      "1 79205.609375 0.4501\n",
      "2 73981.984375 0.4002\n",
      "3 76228.421875 0.3503\n",
      "4 79613.0078125 0.3004\n",
      "5 75639.3984375 0.2505\n",
      "6 72999.71875 0.2006\n",
      "7 72512.5234375 0.1507\n",
      "8 72980.5625 0.1008\n",
      "9 77856.9375 0.0509\n"
     ]
    }
   ],
   "source": [
    "getfile(200, 5).wv.save_word2vec_format('cbow_w2v2.txt', binary=False)\n",
    "file2 = \"/content/cbow_w2v2.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JPz-Hh-hUwXO",
    "outputId": "13dfb5cb-34f9-43a5-bcd0-137f0eb39c8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 187072.546875 0.5\n",
      "1 77459.2734375 0.4501\n",
      "2 77861.015625 0.4002\n",
      "3 74688.4296875 0.3503\n",
      "4 75916.9140625 0.3004\n",
      "5 72696.109375 0.2505\n",
      "6 72054.609375 0.2006\n",
      "7 72795.1875 0.1507\n",
      "8 68752.4453125 0.1008\n",
      "9 71864.234375 0.0509\n"
     ]
    }
   ],
   "source": [
    "getfile(300, 5).wv.save_word2vec_format('cbow_w2v3.txt', binary=False)\n",
    "file3 = \"/content/cbow_w2v3.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52_a2eVTDLn2",
    "outputId": "60c062c9-87c9-4a2d-f12f-cf2174b7ba44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 156137.484375 0.5\n",
      "1 78996.8203125 0.4501\n",
      "2 76894.3203125 0.4002\n",
      "3 74469.734375 0.3503\n",
      "4 74225.9140625 0.3004\n",
      "5 73558.09375 0.2505\n",
      "6 74865.1015625 0.2006\n",
      "7 72223.21875 0.1507\n",
      "8 74269.0390625 0.1008\n",
      "9 71925.9296875 0.0509\n"
     ]
    }
   ],
   "source": [
    "getfile(50, 5).wv.save_word2vec_format('cbow_w2v4.txt', binary=False)\n",
    "file4 = \"/content/cbow_w2v4.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9e2E0U55HXFY",
    "outputId": "13c7df83-8bec-4cfc-c721-67528a29a185"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 130379.4765625 0.5\n",
      "1 81311.0703125 0.4501\n",
      "2 73978.3828125 0.4002\n",
      "3 75391.6015625 0.3503\n",
      "4 74657.1796875 0.3004\n",
      "5 73257.7734375 0.2505\n",
      "6 75173.9453125 0.2006\n",
      "7 72767.3359375 0.1507\n",
      "8 71219.1484375 0.1008\n",
      "9 72271.1796875 0.0509\n"
     ]
    }
   ],
   "source": [
    "getfile(100, 10).wv.save_word2vec_format('cbow_w2v5.txt', binary=False)\n",
    "file5 = \"/content/cbow_w2v5.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eXgHyZuIW0iE",
    "outputId": "3e3603e2-ec8a-4669-da2b-992036862753"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'GloVe'...\n",
      "remote: Enumerating objects: 595, done.\u001b[K\n",
      "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 595 (delta 0), reused 1 (delta 0), pack-reused 592\u001b[K\n",
      "Receiving objects: 100% (595/595), 222.33 KiB | 940.00 KiB/s, done.\n",
      "Resolving deltas: 100% (338/338), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/stanfordnlp/GloVe.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0IVj4wMXja0n"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "VAg6l9S5W-8e"
   },
   "outputs": [],
   "source": [
    "def evaluate_vectors(W, vocab, prefix='./eval/question-data/'):\n",
    "    \"\"\"Evaluate the trained word vectors on a variety of tasks\"\"\"\n",
    "\n",
    "    filenames = [\n",
    "        'capital-common-countries.txt', 'capital-world.txt', 'currency.txt',\n",
    "        'city-in-state.txt', 'family.txt', 'gram1-adjective-to-adverb.txt',\n",
    "        'gram2-opposite.txt', 'gram3-comparative.txt', 'gram4-superlative.txt',\n",
    "        'gram5-present-participle.txt', 'gram6-nationality-adjective.txt',\n",
    "        'gram7-past-tense.txt', 'gram8-plural.txt', 'gram9-plural-verbs.txt',\n",
    "        ]\n",
    "\n",
    "    # to avoid memory overflow, could be increased/decreased\n",
    "    # depending on system and vocab size\n",
    "    split_size = 100\n",
    "\n",
    "    correct_sem = 0; # count correct semantic questions\n",
    "    correct_syn = 0; # count correct syntactic questions\n",
    "    correct_tot = 0 # count correct questions\n",
    "    count_sem = 0; # count all semantic questions\n",
    "    count_syn = 0; # count all syntactic questions\n",
    "    count_tot = 0 # count all questions\n",
    "    full_count = 0 # count all questions, including those with unknown words\n",
    "\n",
    "    for i in range(len(filenames)):\n",
    "        with open('%s/%s' % (prefix, filenames[i]), 'r') as f:\n",
    "            full_data = [line.rstrip().split(' ') for line in f]\n",
    "            full_count += len(full_data)\n",
    "            data = [x for x in full_data if all(word in vocab for word in x)]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            print(\"ERROR: no lines of vocab kept for %s !\" % filenames[i])\n",
    "            print(\"Example missing line:\", full_data[0])\n",
    "            continue\n",
    "\n",
    "        indices = np.array([[vocab[word] for word in row] for row in data])\n",
    "        ind1, ind2, ind3, ind4 = indices.T\n",
    "\n",
    "        predictions = np.zeros((len(indices),))\n",
    "        num_iter = int(np.ceil(len(indices) / float(split_size)))\n",
    "        for j in range(num_iter):\n",
    "            subset = np.arange(j*split_size, min((j + 1)*split_size, len(ind1)))\n",
    "\n",
    "            pred_vec = (W[ind2[subset], :] - W[ind1[subset], :]\n",
    "                +  W[ind3[subset], :])\n",
    "\n",
    "            #cosine similarity if input W has been normalized\n",
    "            dist = np.dot(W, pred_vec.T)\n",
    "\n",
    "\n",
    "            for k in range(len(subset)):\n",
    "                dist[ind1[subset[k]], k] = -np.Inf\n",
    "                dist[ind2[subset[k]], k] = -np.Inf\n",
    "                dist[ind3[subset[k]], k] = -np.Inf\n",
    "\n",
    "            # predicted word index\n",
    "            predictions[subset] = np.argmax(dist, 0).flatten()\n",
    "\n",
    "        \n",
    "        val = (ind4 == predictions) # correct predictions\n",
    "        count_tot = count_tot + len(ind1)\n",
    "        correct_tot = correct_tot + sum(val)\n",
    "        if i < 5:\n",
    "            count_sem = count_sem + len(ind1)\n",
    "            correct_sem = correct_sem + sum(val)\n",
    "        else:\n",
    "            count_syn = count_syn + len(ind1)\n",
    "            correct_syn = correct_syn + sum(val)\n",
    "\n",
    "        print(\"%s:\" % filenames[i])\n",
    "        print('ACCURACY TOP1: %.2f%% (%d/%d)' %\n",
    "            (np.mean(val) * 100, np.sum(val), len(val)))\n",
    "        \n",
    "    return correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ohrGZyVhjas5"
   },
   "outputs": [],
   "source": [
    "def process(file):\n",
    "  with open(file, 'r') as f:\n",
    "    vectors = {}\n",
    "    for line in f.readlines()[1:]: # we only need the embedding vectors starting from the second line\n",
    "        \n",
    "      vals = line.rstrip().split(' ')\n",
    "      try:\n",
    "        vectors[vals[0]] = [float(x) for x in vals[1:]]\n",
    "      except ValueError:\n",
    "        break\n",
    "\n",
    "  vocab_words=list(vectors.keys())\n",
    "  vocab_size = len(vocab_words)\n",
    "  print(\"Vocab size: \",str(vocab_size))\n",
    "\n",
    "  # create word->index and index->word converter\n",
    "  vocab = {w: idx for idx, w in enumerate(vocab_words)}\n",
    "  ivocab = {idx: w for idx, w in enumerate(vocab_words)}\n",
    "\n",
    "  # create the embedding matrix of shape (vocab_size, dim)\n",
    "  vector_dim = len(vectors[ivocab[0]])\n",
    "  W = np.zeros((vocab_size, vector_dim))\n",
    "  for word, v in vectors.items():\n",
    "      if word == '<unk>':\n",
    "          continue\n",
    "      W[vocab[word], :] = v\n",
    "\n",
    "  # normalize each word vector to unit length\n",
    "  # Vectors are usually normalized to unit length before they are used for similarity calculation, making cosine similarity and dot-product equivalent.\n",
    "  W_norm = np.zeros(W.shape)\n",
    "  d = (np.sum(W ** 2, 1) ** (0.5))\n",
    "  W_norm = (W.T / d).T\n",
    "\n",
    "  correct_sem, correct_syn, correct_tot, count_sem, count_syn, count_tot, full_count = evaluate_vectors(W_norm, \n",
    "                                                                                                        vocab, prefix='/content/GloVe/eval/question-data')\n",
    "  print('Questions seen/total: %.2f%% (%d/%d)' %\n",
    "      (100 * count_tot / float(full_count), count_tot, full_count))\n",
    "  print('Semantic accuracy: %.2f%%  (%i/%i)' %\n",
    "      (100 * correct_sem / float(count_sem), correct_sem, count_sem))\n",
    "  print('Syntactic accuracy: %.2f%%  (%i/%i)' %\n",
    "      (100 * correct_syn / float(count_syn), correct_syn, count_syn))\n",
    "  print('Total accuracy: %.2f%%  (%i/%i)' % (100 * correct_tot / float(count_tot), correct_tot, count_tot))\n",
    "\n",
    "  return \n",
    "\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IrT6gsJxFAfr",
    "outputId": "2c99ebb4-af16-48a1-fba6-5e7eefb0f958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  22433\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 0.00% (0/272)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 0.00% (0/562)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 0.00% (0/28)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 0.00% (0/545)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 0.00% (0/342)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 0.00% (0/812)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 0.00% (0/506)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 0.08% (1/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 0.00% (0/849)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1482)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 0.10% (1/992)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "Questions seen/total: 52.32% (10226/19544)\n",
      "Semantic accuracy: 0.00%  (0/1749)\n",
      "Syntactic accuracy: 0.02%  (2/8477)\n",
      "Total accuracy: 0.02%  (2/10226)\n"
     ]
    }
   ],
   "source": [
    "process(ofile)  # (100,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZyqX2qMFlPjh",
    "outputId": "fe6549ec-ed6d-4930-f7ad-3828e176ad68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  22433\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 0.00% (0/272)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 0.00% (0/562)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 0.00% (0/28)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 0.18% (1/545)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 0.00% (0/342)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 0.00% (0/812)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 0.00% (0/506)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 0.00% (0/849)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1482)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "Questions seen/total: 52.32% (10226/19544)\n",
      "Semantic accuracy: 0.06%  (1/1749)\n",
      "Syntactic accuracy: 0.00%  (0/8477)\n",
      "Total accuracy: 0.01%  (1/10226)\n"
     ]
    }
   ],
   "source": [
    "process(file2)  # (200, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BIxVm2KqAolc",
    "outputId": "116082cb-ee1b-46d7-86bb-2a0f64b3b108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  22433\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 0.00% (0/272)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 0.00% (0/562)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 0.00% (0/28)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 0.18% (1/545)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 0.00% (0/342)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 0.12% (1/812)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 0.00% (0/506)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 0.08% (1/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 0.00% (0/849)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1482)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "Questions seen/total: 52.32% (10226/19544)\n",
      "Semantic accuracy: 0.06%  (1/1749)\n",
      "Syntactic accuracy: 0.02%  (2/8477)\n",
      "Total accuracy: 0.03%  (3/10226)\n"
     ]
    }
   ],
   "source": [
    "process(file3) # (300,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zek1J5VtDSXa",
    "outputId": "f3fc2221-ba48-4f4d-c1a9-9d498b24c73b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  22433\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 0.00% (0/272)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 0.00% (0/562)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 0.00% (0/28)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 0.00% (0/545)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 0.00% (0/342)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 0.00% (0/812)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 0.00% (0/506)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 0.00% (0/849)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1482)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "Questions seen/total: 52.32% (10226/19544)\n",
      "Semantic accuracy: 0.00%  (0/1749)\n",
      "Syntactic accuracy: 0.00%  (0/8477)\n",
      "Total accuracy: 0.00%  (0/10226)\n"
     ]
    }
   ],
   "source": [
    "process(file4) # (50,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qixhKXYmHiQJ",
    "outputId": "52fa0939-4a92-4b12-bd36-9ef5a760e552"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  22433\n",
      "capital-common-countries.txt:\n",
      "ACCURACY TOP1: 0.00% (0/272)\n",
      "capital-world.txt:\n",
      "ACCURACY TOP1: 0.00% (0/562)\n",
      "currency.txt:\n",
      "ACCURACY TOP1: 0.00% (0/28)\n",
      "city-in-state.txt:\n",
      "ACCURACY TOP1: 0.00% (0/545)\n",
      "family.txt:\n",
      "ACCURACY TOP1: 0.00% (0/342)\n",
      "gram1-adjective-to-adverb.txt:\n",
      "ACCURACY TOP1: 0.00% (0/812)\n",
      "gram2-opposite.txt:\n",
      "ACCURACY TOP1: 0.00% (0/506)\n",
      "gram3-comparative.txt:\n",
      "ACCURACY TOP1: 0.08% (1/1332)\n",
      "gram4-superlative.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "gram5-present-participle.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram6-nationality-adjective.txt:\n",
      "ACCURACY TOP1: 0.00% (0/849)\n",
      "gram7-past-tense.txt:\n",
      "ACCURACY TOP1: 0.00% (0/1482)\n",
      "gram8-plural.txt:\n",
      "ACCURACY TOP1: 0.00% (0/992)\n",
      "gram9-plural-verbs.txt:\n",
      "ACCURACY TOP1: 0.00% (0/756)\n",
      "Questions seen/total: 52.32% (10226/19544)\n",
      "Semantic accuracy: 0.00%  (0/1749)\n",
      "Syntactic accuracy: 0.01%  (1/8477)\n",
      "Total accuracy: 0.01%  (1/10226)\n"
     ]
    }
   ],
   "source": [
    "process(file5) # (100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KEW1zMgVMREr"
   },
   "source": [
    "## 3.2. Performance Evaluation\n",
    "\n",
    "\n",
    "You are required to provide the table with precision, recall, f1 of test set.\n",
    "Note that it will not be marked if you do not display it in the ipynb file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVCF0bwTtRS0"
   },
   "source": [
    "(*Please show your empirical evidence*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPHCb-bneTI9"
   },
   "outputs": [],
   "source": [
    "# Please comment your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P28Z1k36MZuo"
   },
   "source": [
    "## 3.3. Hyperparameter Testing\n",
    "*You are required to draw a graph(y-axis: f1, x-axis: epoch) for test set and explain the optimal number of epochs based on the learning rate you have already chosen.* Note that it will not be marked if you do not display it in the ipynb file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BYzrA_s2tTaz"
   },
   "source": [
    "(*Please show your empirical evidence*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTLyQEeZMZ2f"
   },
   "outputs": [],
   "source": [
    "# Please comment your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfv8rWTKPzeb"
   },
   "source": [
    "## Object Oriented Programming codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TS23AjBRSZaX"
   },
   "source": [
    "*You can use multiple code snippets. Just add more if needed* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hVmx4E52dXS"
   },
   "outputs": [],
   "source": [
    "# If you used OOP style, use this section"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "sfv8rWTKPzeb"
   ],
   "name": "Copy of UNIKEY_COMP5046_Ass1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
